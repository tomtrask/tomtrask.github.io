<!DOCTYPE html>
<html>
  <head>
    <title>How the Predictions Were Tested</title>
    <link rel = "stylesheet"
          type = "text/css"
          href = "res/css/myStyle.css" />
        
  </head>
  <body>
    <h3>Organization</h3>
    <p>
      My original objective in undertaking this tests was simply to determine
      if some freely available information had or has any discernible value.
      At this point, about a month into my second MLB season, I'd have to say
      the answer is maybe, maybe not.
    </p>
    <h3>How Gambling Works and How I Talk About It</h3>
    <p>
      Today, somewhere, some individual player or a team will face off against
      another player or team in a contest that has rules that prevent ties - one
      or the other player or team will win. Betting invariably occurs, I only
      concern myself with that gambling done through a bookmaker. Sides of this
      contest might have names like Alpha Anacondas or Bravo Berries but I'll
      stick with letter names - A and B. A bookmaker takes bets on A and B and
      with each bet offers odds for the payout that depend on the amount of
      money that bookmaker has taken in for either side of the contest. If
      bettors have put $100 on both sides, the next bet will get even money
      something approximating even money odds. Even money is 1:1 in math class
      but this bookmaker is, in fact, in it for the money so she will offer
      odds, or payout of 10:11 on both teams and call it even money. A payout
      of 10:11 means that if you bet $11, you could walk away with $21 if your
      side wins or $0 if they loose. There is an implied probability of a win
      that goes with those payouts. In the case of a 10:11 payout, the odds of
      either team winning are 11:10 or the probability is 11/21 or 52.3%.
      Whichever side wins, the bookmaker will pocket about $1 per $22 bet. This
      is called vigorish or vig but that word has a negative connotation like
      it's the bookmaker somehow cheating the bettors out of their proper
      winnings. In fact, it's simply the fee the bettor pays for operating an
      orderly market. The payouts a bookmaker offers should always result in
      the bookmaker having a neutral position on any contest. If a bookmaker is
      offering even money on A, even after 2/3 of the bets have been placed on
      A, then that bookmaker is offering a higher payout than the betting would
      support and they then have a long position on B. And that's fine...but
      it's not something we would know about and it's not something we would
      expect.
    </p>
    <p>
      Payouts can be expressed in different "units" but all those units can be
      converted to an implied probability or probability and I want to label
      that probability as a price of a bet. In fact the odds offered by a
      bookmaker are the ask price for a contest. Independent of that valuation,
      I may have another probability of a side winning that is above or below
      that ask price. If my valuation is greater than the price, I can compute
      a bet, f_star, with this implementation of the the
      <a href="https://en.wikipedia.org/wiki/Kelly_criterion">Kelly criterion
      </a>.
      <div class="code_block">
      <code>f_star = p * (b - 1) + 1</code><br>
      Where:<br>
      <code>p</code> = my valuation, or probability of win<br>
      <code>b</code> = payoff of a single dollar bet
      (<code>(1-price)/price</code>)<br>
      <code>f_star</code> = the fraction of some nominal bet one should wager
      on this side
      </div>
      Kelly criterion lets us tie up gambling in a nice neat economic package.
      Wagers are a simple, game-theoretical, proposition that my estimate of the
      value of a good (a win) exceeds the market price. The greater the
      disparity between price and valuation, the more I will demand (wager).
      The value that Kelly adds is simply to tell us how much of our stake to
      wager on a given game (much like asset allocation in securities). This
      isn't a perfect analogy though. It explains demand but not supply. From
      the sell-side, gambling is simply a matter of balancing demands for
      competitive commodity products. Too much demand for product A and the
      price of A will rise and B will decline. So from the bookmaker's side the
      better model would be that of a bank. If loan demand is high, savings
      rates will rise. If there's a surplus of savings over loan demand, savings
      rates will fall (and so will loan rates - loan rates will never be
      lower than savings rates).
    </p>
    <h3>Seasonal Results</h3>
    <h4>2018 MLB</h4>
    <p>
      The 2018 MLB season was the first season I played with. By play I merely
      mean that I scraped game odds and results from oddsshark and made paper
      bets based on those values.
    </p>
    <p>
      Sorry, I shouldn't say scraped. I went out every morning about 9AM West
      Coast and copied all the values into a csv file. I also used an mlb public
      API to get game details and correlate those details back with the
      (manually) scraped betting details. It was this manual scraping that
      accounts for some holes in my database. Ultimately, I did get details for
      1901 games and bet on 1869 (98.3%) of those games. We bet both sides of
      four games so the total number of bets is 1873. Paper bets starting with
      a stake of $10,000 look like figure 1.
    </p>
    <div style="width:100%">
      <img style="display:block;margin-left:auto;margin-right:auto;" src="res/images/MLB_2018_stake_history.png">
      <p style="width:400px:margin-left:auto;margin-right:auto;">
        Figure 1: Stake value over span of 2018 MLB season
      </p>
    </div>
    <p>
      What can we make of this? It appears that from day 1 to day 130 we didn't
      benefit from betting at all. The final 3 weeks of the season seem to be
      the one time where the predictions vary enough from the market pricing
      that we can capitalize on those predictions. The red line is an
      exponential curve fit to the actual stake and the right hand intercept is
      about $27465.
    </p>
    <h4>Late 2018-2019 NBA</h4>
    <p>
      I started tracking this season late and only engaged because I figured out
      a way to scrape the odds from the oddsshark site without having to view
      the page daily. Now I have a clean set of daily odds and predictions for
      the period after February 5, 2019. Add to that the fact that basketball
      is much higher than the standard deviation of a side's score (mean ~90,
      sd ~12-13). That is not true of baseball (mean score 4.547, sd 3.212). 
    </p>
    <p>
      I'm going to have to come back to this.
    </p>
    <h4>2019 MLB</h4>
    <p>
      The 2019 baseball predictions have been nothing short of abysmal and this
      is a good thing. As of May 3, I was down about 83% even though the
      predictions indicated that the sides it picked would win ~65% of the
      time. It's pretty obvious our estimates are biased. On any given day we
      can compute our CI of our predictions like so:
      <div class="code_block">
        <code>
          num_games_played = len(games_played)<br>
          avg_prob_win = sum(game["prob_win"])/num_games_played<br>
          p = avg_prob_win<br>
          std_err = math.sqrt(p*(1-p)/num_games_played)<br>
          z = 1.96 # 95% CI<br>
          ci = (p-z*std_err, p+z*std_err)<br>
        </code>
      </div>
    </p>
    <p>
      As of May 1 our 95% CI is (.608, .698) and our average experienced win
      rate is 0.466. So problem solved, right? Our predicted scores suck
      because they overestimate the likelihood of a win.
    </p>
    <p>
      Or not? Our experienced rate not only significantly below prediction, it
      was below 50%. I suspect that as long as experienced win rate is above
      50% any strategy performs fine. One way to test this would be to run
      paper bets against the favorite side of all games, assuming the predicted
      probability is 90%. This presumes that the market favorite is, in fact,
      more likely to win - we should demonstrate this. This is all future work.
      One thing I can say today is that our experienced win rate was
      significantly below our predicted rate since game 29 of the season. Game
      29 was played on day 3 of the season. Hypothetically we could bet until
      our predictions are clearly out of control. That means, in this case, we
      would have bet on all relevant games in the first three days of the
      season and then stopped betting. Our net gain for this season would be
      $-1038 or about a 10% loss. It seems bad for a 3 day season but it's much
      better than hanging on to a losing strategy.
    </p>
    <h4>Are Risk Markets Efficient and What Does That Mean for Me?</h4>
    <p>
      I've heard markets are the most efficient way to determine price but
      given the way I define value and price in gambling (at a bookmaker which
      is, as far as I can tell, a market). Now I have a tool, confidence
      interval, to precisely tell me how far my valuations are from reality and
      I've gotta say, it is not pretty. We already saw how bad oddsshark
      predictions are from actual results. Dig this. When I value a side at
      the oddsshark odds less vigorish (i.e. sum of both teams implied
      probabilities is 1) the average price of all favorites in 2018 was
      58.88%. Among those favorite sides 57.87% went on to win the game. SE of
      favorite teams was 1.131% so the market was within 1 SE of the final,
      experienced win rate.
    </p>
    <h3>Conclusions</h3>
    <p>
      What I've learned in this experimentation can be boiled down to two main
      points.
    </p>
    <p>
      First, whenever I've tried to model things I wind up, at some point,
      computing a Pearson's correlation coefficient and it tells me that my
      model explains so much of the error in the observations that went into
      creating that model. It's that litmus test that says "this model probably
      does what you want" or not. And of course it's nothing to come up with a
      model that has a very low Pearson's coefficient and still advertise the
      model results. That's what Oddsshark is doing, that's what most public
      predictions entail (the DJIA is going to 36k, the DJIA is going to 10k,
      gold is going to $5k, oil is going to $150/bbl). Oddsshark is predicting
      things with a schedule (to their credit) but their predictions are
      ephemeral and there's no record to hold them to their predictions after
      the fact. I've attempted to construct such a record and stumbled across
      at least one method of confirming that their predictions have value (they
      do not). The good news is that even though oddsshark has hidden the
      method by which they make projections, we can arrive at an evaluation of
      their projections in a relatively short timeframe within a single season.
      So buyer beware - and here's how.
    </p>
    <p>
      Second, I now have an actual practical understanding of the claim about
      markets being the best estimate of probability. Best estimate in this
      claim simply means the least biased. It's easy to see how to measure this
      bias in the case of betting odds - they convert simply into probabilities.
      What's harder is to see bias between score predictions and win rates or
      differences between predicted scores and actual odds. In the end, it's
      only win/loss ratios that pay so even if you could demonstrate that
      predicted scores are unbiased (have the same mean as actual scores) it
      wouldn't matter to the outcome of our test nor would it be comparable to
      the amount of bias in the odds market's odds setting.
    </p>
  </body>
</html>
